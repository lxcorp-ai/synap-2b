# Synap-2b

<img src="https://huggingface.co/lxcorp/Synap-2b/resolve/main/synap_logo.png" width="400" align="center">

## ğŸ“– Sobre o Modelo

**Synap-2b** Ã© um modelo de linguagem de **2 bilhÃµes de parÃ¢metros**, desenvolvido pela **Î»Ï‡ Corp. (Marius Jabami)**.  
Ele foi projetado para **geraÃ§Ã£o de texto**, **raciocÃ­nio matemÃ¡tico** e **pesquisa em LLMs**, com suporte a **PortuguÃªs** e **InglÃªs**.  

âš ï¸ Este modelo Ã© **privado** e nÃ£o estÃ¡ disponÃ­vel para download, mas pode ser acessado via Space na Hugging Face.

---

## ğŸš€ Como Usar

```python
import torch
from transformers import pipeline

model_id = "lxcorp/Synap-2b"

pipe = pipeline(
    "text-generation",
    model=model_id,
    torch_dtype=torch.bfloat16,
    device_map="auto"
)

text = pipe("Resolva: 2x + 5 = 15")
print(text)
```

---

## ğŸ§  Detalhes TÃ©cnicos

ParÃ¢metros: 2B

Linguagens: PortuguÃªs, InglÃªs

Dataset: orca-math-portuguese-64k

## Arquitetura:

DimensÃ£o do modelo: 2048

MLP: 8192

Camadas: 28

Heads: 16

Contexto: 4096 tokens




---

ğŸ“Š **AvaliaÃ§Ã£o**

| Benchmark | Synap-2b |
|-----------|----------|
| MMLU      | 52.0     |
| ARC E     | 82.2     |
| ARC C     | 64.6     |
| PIQA      | 78.5     |
| SIQA      | 62.3     |
| **MÃ©dia** | **61.1** |



---

## âš¡ Hardware & Treinamento

Treinado em GPUs NVIDIA (detalhes internos privados).

## Framework: Transformers.

TÃ©cnicas: Fine-tuning em dataset educacional + ajustes para raciocÃ­nio matemÃ¡tico.



---

## ğŸ“œ LicenÃ§a

Modelo disponibilizado sob licenÃ§a CC, uso controlado e privado.


---

## âœï¸ CitaÃ§Ã£o

@misc{synap2b2025,
  author = {Marius Jabami},
  title = {Synap-2b: Fine-tuned 2B Language Model},
  year = {2025},
  howpublished = {\url{https://huggingface.co/lxcorp/Synap-2b}}
}


---

Desenvolvido com â¤ï¸ por Marius Jabami â€¢ Î»Ï‡ Corp.